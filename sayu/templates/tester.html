{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Three.js VRM Viewer</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap');
        
        :root {
            --primary-color: #7e57c2;
            --secondary-color: #4caf50;
            --accent-color: #ff9800;
            --bg-color: #f9f9f9;
            --card-color: #ffffff;
            --text-color: #333333;
            --shadow: 0 8px 24px rgba(0,0,0,0.13);
            --border-radius: 2rem;
            --button-gradient: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
            --button-hover-gradient: linear-gradient(135deg, #38f9d7 0%, #43e97b 100%);
        }
        
        * {
            box-sizing: border-box;
            transition: all 0.3s ease;
        }
        
        body { 
            margin: 0; 
            overflow: hidden; 
            display: flex; 
            flex-direction: column; 
            align-items: center; 
            justify-content: center; 
            height: 100vh; 
            background-color: var(--bg-color);
            font-family: 'Poppins', sans-serif;
            color: var(--text-color);
        }
        
        #viewer-container {
            background: linear-gradient(135deg, #27ae60, #2ecc71);
            width: 25vw;
            height: 50vh;
            border-radius: var(--border-radius);
            margin: 2vh 2vw;
            display: flex;
            align-items: center;
            justify-content: center;
            box-shadow: var(--shadow);
            overflow: hidden;
            border: 4px solid white;
        }
        
        canvas {
            display: block; 
            border-radius: calc(var(--border-radius) - 5px);
        } 
        
        .main-box {
            display: flex;
            flex-direction: row;
            width: 100vw;
            height: 100vh;
            background: var(--bg-color);
            padding: 1rem;
        }
.call-box {
    background: linear-gradient(135deg, #3498db, #2980b9);
    width: 60vw;
    height: 90vh;
    margin: 2vh 2vw;
    border-radius: var(--border-radius);
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    box-shadow: var(--shadow);
    border: 4px solid white;
    position: relative;
    overflow: hidden;
}
#videoCallLayout, #normalCallLayout {
    width: 100%;
    height: 100%;
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
}
#localVideo, #remoteVideo {
    width: 90%;
    max-height: 40%;
    margin: 1vh 0;
    background: #222;
    border-radius: 1vw;
    border: 2px solid #fff;
    box-shadow: var(--shadow);
}
.avatar-placeholder {
    font-size: 8vw;
    margin-bottom: 2vh;
}
.call-status {
    font-size: 2vw;
    color: #fff;
    background: #2ecc71;
    padding: 1vw 2vw;
    border-radius: 1vw;
    box-shadow: var(--shadow);
    border: 2px solid white;
}
.emotion-score-con {
    background: linear-gradient(135deg, #f39c12, #e67e22);
    width: 30vw;
    height: 80vh;
    border-radius: var(--border-radius);
    display: flex;
    flex-direction: column;
    align-items: flex-start;
    justify-content: flex-start;
    padding: 2vw 1vw;
    gap: 1vw;
    position: absolute;
    top: 5vh;
    right: 2vw;
    transform: translate(0, 0);
    z-index: 10;
    box-shadow: var(--shadow);
    border: 4px solid white;
}   
.emotion-bar {
    width: 100%;
    height: 2vw;
    background: #fff;
    border-radius: 1vw;
    margin-bottom: 1vw;
    display: flex;
    align-items: center;
    overflow: hidden;
    box-shadow: 0 2px 6px rgba(0,0,0,0.1);
    border: 1px solid rgba(255,255,255,0.8);
}
.emotion-bar-inner {
    height: 1.5vh;
    border-radius: 0.3vw;
    transition: width 0.5s ease;
    box-shadow: 0 0 10px rgba(255,255,255,0.3);
}
.emotion-label {
    margin-left: 0.5vw;
    font-size: 0.8vw;
    font-weight: 500;
    color: #fff;
    text-shadow: 0 1px 2px rgba(0,0,0,0.3);
}
#viewer-container {
    background: #27ae60; /* green */
    width: 25vw;
    height: 50vh;
    border-radius: 3vw;
    margin: 2vh 2vw;
    display: flex;
    align-items: center;
    justify-content: center;

}

.button-box-con {
    display: flex;
    position: fixed;
    flex-direction: column;
    gap: 2vh;
    margin-top: 2vh;
    transform: translateY(-40vh);
    margin-bottom: 2vh;
    align-items: flex-start;
    z-index: 30;
}
.call-button {
    display: flex;
    align-items: center;
    justify-content: center;
    width: 22vw;
    min-width: 220px;
    height: 8vh;
    min-height: 56px;
    background: var(--button-gradient);
    border-radius: var(--border-radius);
    padding: 0;
    margin: 0;
    z-index: 20;
    cursor: pointer;
    box-shadow: var(--shadow);
    border: none;
    transform: translateY(0);
    transition: transform 0.2s, box-shadow 0.2s, background 0.2s;
    position: relative;
    overflow: hidden;
}
.call-button:hover {
    background: var(--button-hover-gradient);
    transform: translateY(-4px) scale(1.03);
    box-shadow: 0 16px 32px rgba(0,0,0,0.13);
}
.call-button header {
    font-size: 2.1vw;
    font-weight: 600;
    color: #fff;
    text-align: center;
    margin: 0 auto;
    letter-spacing: 0.5px;
    line-height: 8vh;
    width: 100%;
    display: block;
    text-shadow: 0 2px 8px rgba(0,0,0,0.13);
    transition: font-size 0.2s, letter-spacing 0.2s;
    user-select: none;
}
.call-button:hover header {
    font-size: 2.3vw;
    letter-spacing: 1.5px;
}
.text-box {
    background: rgba(255,255,255,0.7);
    color: #7e57c2;
    font-size: 1.1vw;
    font-weight: 500;
    border-radius: 1rem;
    padding: 1vw 1.5vw;
    margin-top: 1vw;
    box-shadow: 0 2px 8px rgba(0,0,0,0.08);
    width: 90%;
    min-height: 2vw;
    display: flex;
    align-items: center;
    overflow: hidden;
}
.text-display {
    width: 100%;
    height: 15vh;
    overflow-y: auto;
    padding: 0.5vw;
    background: rgba(255,255,255,0.3);
    border-radius: 0.5rem;
    font-size: 0.9vw;
    line-height: 1.4;
}
.user-message {
    background: #e3f2fd;
    padding: 0.3vw 0.8vw;
    margin: 0.2vw 0;
    border-radius: 0.5rem;
    border-left: 3px solid #2196f3;
}
.ai-message {
    background: #f3e5f5;
    padding: 0.3vw 0.8vw;
    margin: 0.2vw 0;
    border-radius: 0.5rem;
    border-left: 3px solid #9c27b0;
}
.speech-indicator {
            position: absolute;
            top: 2vh;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(46, 204, 113, 0.9);
            color: white;
            padding: 0.5vw 1vw;
            border-radius: 1vw;
            font-size: 0.9vw;
            font-weight: 500;
            z-index: 20;
            transition: all 0.3s ease;
            backdrop-filter: blur(5px);
        }
        
        .speech-indicator.speaking {
            background: rgba(231, 76, 60, 0.9);
            animation: pulse 1s infinite;
        }
        
        @keyframes pulse {
            0% { transform: translateX(-50%) scale(1); }
            50% { transform: translateX(-50%) scale(1.05); }
            100% { transform: translateX(-50%) scale(1); }
        }
        
        .live-indicator {
            position: absolute;
            top: 1vh;
            right: 2vw;
            background: #e74c3c;
            color: white;
            padding: 0.3vw 0.8vw;
            border-radius: 0.5vw;
            font-size: 0.8vw;
            font-weight: 600;
            z-index: 25;
            animation: blink 2s infinite;
        }
        
        @keyframes blink {
            0%, 50% { opacity: 1; }
            51%, 100% { opacity: 0.5; }
        }
        
        .voice-wave {
            position: absolute;
            bottom: 2vh;
            left: 50%;
            transform: translateX(-50%);
            width: 60%;
            height: 4vh;
            background: rgba(255,255,255,0.1);
            border-radius: 2vh;
            overflow: hidden;
            backdrop-filter: blur(5px);
        }
        
        .emotion-bar {
            display: flex;
            align-items: center;
            margin: 0.3vw 0;
            padding: 0.2vw;
            background: rgba(255,255,255,0.1);
            border-radius: 0.5vw;
            transition: all 0.3s ease;
        }
        
        .emotion-bar:hover {
            background: rgba(255,255,255,0.2);
            transform: scale(1.02);
        }
        
        .emotion-bar-inner {
            height: 1.5vh;
            border-radius: 0.3vw;
            transition: width 0.5s ease;
            box-shadow: 0 0 10px rgba(255,255,255,0.3);
        }
        
        .emotion-label {
            margin-left: 0.5vw;
            font-size: 0.8vw;
            font-weight: 500;
            color: #fff;
            text-shadow: 0 1px 2px rgba(0,0,0,0.3);
        }
    </style>
    <script type="importmap">
        {
            "imports": {
                "three": "https://cdn.jsdelivr.net/npm/three@0.176.0/build/three.module.js",
                "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.176.0/examples/jsm/",
                "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3/lib/three-vrm.module.min.js"
            }
        }
    </script>
</head>
<body>
    {% csrf_token %}
    <div class="button-box-con">
        <div class="call-button" id="audioCallBtn">
        <header>start a call</header>
            </div>
        <div class="call-button" id="videoCallBtn">
            <header>start a video call</header>
        </div>
    </div>
     
    <div class="main-box">

      
        <!-- Call Area -->
        <div class="call-box call-modal" id="callBox">
            <button id="closeCallBox" style="position:absolute;top:1vw;right:1vw;z-index:30;background:rgba(255,255,255,0.3);color:white;border:none;border-radius:50%;width:3vw;height:3vw;font-size:1.5vw;cursor:pointer;box-shadow:var(--shadow);backdrop-filter:blur(5px);">‚úñ</button>
            
            <!-- Video Call Layout -->
            <div id="videoCallLayout" style="width: 100%; height: 100%; display: flex; flex-direction: row; align-items: center; justify-content: space-between;">
                <div style="display: flex; flex-direction: column; align-items: center; width: 45%; position: relative;">
                    <div class="live-indicator">üî¥ LIVE</div>
                    <video id="localVideo" autoplay playsinline muted style="width: 100%; max-height: 35vh; background: #222; border-radius: 1vw; border: 2px solid #fff; box-shadow: var(--shadow);"></video>
                    <div class="call-status" id="videoCallStatus" style="margin-top: 1vh; font-size: 1.2vw; color: #fff; background: #2ecc71; padding: 0.5vw 1vw; border-radius: 0.5vw;">Video Call Active - Analyzing emotions...</div>
                    <canvas id="emotionCanvas" style="display: none;"></canvas>
                    <form method="post" action="/stop-ai-session/" style="margin-top: 1vh;">
                        {% csrf_token %}
                        <input type="hidden" name="action" value="stop_session">
                        <input type="hidden" name="force_stop" value="true">
                        <button type="submit" id="stopVideoCall" style="padding: 0.8vh 1.5vw; background: #e74c3c; color: white; border: none; border-radius: 0.8vw; cursor: pointer; box-shadow: var(--shadow);">Stop Video Call</button>
                    </form>
                </div>
                <!-- 3D Viewer -->
                <div id="viewer-container" style="width: 40%; max-height: 35vh; margin: 1vh; background: linear-gradient(135deg, #27ae60, #2ecc71); border-radius: 1vw; border: 2px solid #fff; box-shadow: var(--shadow); overflow: hidden; transform: translateX(-3vw); position: relative;">
                    <div class="speech-indicator" id="speechIndicatorVideo" style="display: none;">üéôÔ∏è Listening...</div>
                </div>
            </div>
            
            <!-- Normal Call Layout (audio only) -->
            <div id="normalCallLayout" style="display:none; width: 100%; height: 100%; display: flex; flex-direction: row; align-items: center; justify-content: space-between;">
                <div style="display: flex; flex-direction: column; align-items: center; width: 45%; position: relative;">
                    <div class="live-indicator">üî¥ LIVE</div>
                    <div class="avatar-placeholder" style="font-size: 6vw; margin-bottom: 2vh;">üé§</div>
                    <div class="call-status" id="audioCallStatus" style="font-size: 1.2vw; color: #fff; background: #2ecc71; padding: 0.5vw 1vw; border-radius: 0.5vw;">Audio Call Active - Voice analysis enabled</div>
                    <canvas id="audioVisualization" style="width: 80%; height: 10vh; background: #333; border-radius: 1vw; margin: 2vh 0; border: 2px solid #fff;"></canvas>
                    <div class="voice-wave"></div>
                    <form method="post" action="/stop-ai-session/" style="margin-top: 1vh;">
                        {% csrf_token %}
                        <input type="hidden" name="action" value="stop_session">
                        <input type="hidden" name="force_stop" value="true">
                        <button type="submit" id="stopAudioCall" style="padding: 0.8vh 1.5vw; background: #e74c3c; color: white; border: none; border-radius: 0.8vw; cursor: pointer; box-shadow: var(--shadow);">Stop Audio Call</button>
                    </form>
                </div>
                <!-- 3D Viewer for virtual ai assistant to appear  -->
                <div id="viewer-container-audio" style="width: 100%; max-height: 35vh; margin: 1vh; background: linear-gradient(135deg, #27ae60, #2ecc71); border-radius: 1vw; border: 2px solid #fff; box-shadow: var(--shadow); overflow: hidden; position: relative;">
                    <div class="speech-indicator" id="speechIndicator" style="display: none;">üéôÔ∏è Listening...</div>
                </div>
            </div>
        </div>
            
        <!-- Emotion Score Bar -->
        <div class="emotion-score-con">
            <div id="emotionScoreCon" style="width: 90%;"></div>
            <div class="text-box">
                <div class="text-display" id="textDisplay">
                    <!-- Conversation between user and AI will be displayed here -->
                </div>
            </div>
            <!-- Bars will be injected here -->
        </div>
        
        </div>
        
        
        
    </div>
    <!-- ...your script tags... -->
    
    <script type="module">
        ////////////////////////////////////////////////////////////////////
        // THREE.JS IMPORTS AND INITIALIZATION
        ////////////////////////////////////////////////////////////////////
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
        import { VRMLoaderPlugin } from '@pixiv/three-vrm';

        ////////////////////////////////////////////////////////////////////
        // GLOBAL VARIABLES AND STATE MANAGEMENT
        ////////////////////////////////////////////////////////////////////
        const viewerContainer = document.getElementById('viewer-container');
        let currentVrm = null;
        let localStream = null;
        let mediaRecorder = null;
        let audioContext = null;
        let isCallActive = false;
        let isVideoCall = false;
        let emotionInterval = null;
        let audioAnalyser = null;
        let isAISpeaking = false; // Keep for compatibility but Hume EVI manages this

        ////////////////////////////////////////////////////////////////////
        // THREE.JS SCENE SETUP AND CONFIGURATION
        ////////////////////////////////////////////////////////////////////
        const scene = new THREE.Scene();
        const camera = new THREE.PerspectiveCamera(75, viewerContainer.clientWidth / viewerContainer.clientHeight, 0.1, 1000);
        const renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(viewerContainer.clientWidth, viewerContainer.clientHeight);
        renderer.domElement.id = "myVrmCanvas";
        renderer.domElement.className = "my-vrm-canvas";  
        viewerContainer.appendChild(renderer.domElement);

        // Add OrbitControls for camera interaction
        const controls = new OrbitControls(camera, renderer.domElement);
        controls.target.set(0, 1.0, 0);
        controls.update();

        // Add lighting environment
        const light = new THREE.DirectionalLight(0xffffff, 1);
        light.position.set(1, 1, 1).normalize();
        scene.add(light);
        scene.add(new THREE.AmbientLight(0x404040));
        scene.background = new THREE.Color(0xcccccc);

        // Set camera position and target
        camera.position.set(0, 0.6, 0.5);
        controls.target.set(0, 0.6, 0);
        controls.update();

        ////////////////////////////////////////////////////////////////////
        // SCENE MANAGEMENT AND ANIMATION LOOP
        ////////////////////////////////////////////////////////////////////
        // Handle window resizing
        window.addEventListener('resize', () => {
            camera.aspect = viewerContainer.clientWidth / viewerContainer.clientHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(viewerContainer.clientWidth, viewerContainer.clientHeight);
        });

         
        const clock = new THREE.Clock();
        function animate() {
            requestAnimationFrame(animate);
            const delta = clock.getDelta();
            if (currentVrm) {
                currentVrm.update(delta);
            }
            
            if (currentVrm) {
                const rightshoulder = currentVrm.humanoid.getRawBoneNode('rightShoulder');
                const rightArm_U = currentVrm.humanoid.getRawBoneNode('rightUpperArm');
                const rightArm_L = currentVrm.humanoid.getRawBoneNode('righLowerArm');
                const rightHand = currentVrm.humanoid.getRawBoneNode('rightHand');
    
                const leftshoulder = currentVrm.humanoid.getRawBoneNode('leftShoulder');
                const leftArm_U = currentVrm.humanoid.getRawBoneNode('leftUpperArm');
                const leftArm_L = currentVrm.humanoid.getRawBoneNode('leftLowerArm');
                const leftHand = currentVrm.humanoid.getRawBoneNode('leftHand');

                const neck = currentVrm.humanoid.getRawBoneNode('neck');

                //currentVrm.update(delta);

                //vrm facial expression control currently default  
                currentVrm.expressionManager.setValue('happy', 0);
                currentVrm.expressionManager.setValue('suprised',0);
                currentVrm.expressionManager.setValue('angry', 0);
                currentVrm.expressionManager.setValue('sad',0 );
                

                //lips movement controls currentky neutral  
                currentVrm.expressionManager.setValue('aa', 0);
                currentVrm.expressionManager.setValue('ee', 0);
                currentVrm.expressionManager.setValue('ou', 0);
                currentVrm.expressionManager.setValue('oh', 0);
                currentVrm.expressionManager.setValue('ih', 0);
                //curraentVrm.expressionManager.setValue('relaxed', 0); do not use 
                
                
                //set arm poostion to normal position , set both values to 0 for "T-pose"
                leftArm_U.rotation.z = -1;
                rightArm_U.rotation.z = 1;
                
                // max neck z range set from -0.2 to 0.2 for tilting 
                neck.rotation.z = 0;

                //spin head on neck 
                neck.rotation.y = 0 ;
                //

            }

            controls.update();
            renderer.render(scene, camera);
            }

            animate();
            

        ////////////////////////////////////////////////////////////////////
        // VRM MODEL LOADING AND SETUP along with animation
        ////////////////////////////////////////////////////////////////////
        const loader = new GLTFLoader();
        loader.register((parser) => new VRMLoaderPlugin(parser));

        // Use Django static tag for VRM path
        const vrmPath = "{% static 'sayu_body/sayu.vrm' %}";

        loader.load(
            vrmPath,
            (gltf) => {
                currentVrm = gltf.userData.vrm;
                scene.add(currentVrm.scene);

                // Center the model
                const box = new THREE.Box3().setFromObject(currentVrm.scene);
                const center = box.getCenter(new THREE.Vector3());
                currentVrm.scene.position.sub(center.multiplyScalar(1));

                console.log('VRM loaded:', currentVrm);
            },
            (progress) => {
                console.log('Loading model...', 100.0 * (progress.loaded / progress.total), '%');
            },
            (error) => {
                console.error('Error loading VRM:', error);
                alert('Failed to load VRM model. Check console for details.');
            }
        );

        ////////////////////////////////////////////////////////////////////
        // FAKE EMOTION DATA FOR TESTING AND DISPLAY
        ////////////////////////////////////////////////////////////////////
        const fakeEmotions = [
            { name: "Happy", score: 0.8, color: "#2ecc71" },
            { name: "Sad", score: 0.3, color: "#3498db" },
            { name: "Angry", score: 0.1, color: "#e74c3c" },
            { name: "Surprised", score: 0.5, color: "#f1c40f" },
            { name: "Fear", score: 0.2, color: "#9b59b6" },
            { name: "Neutral", score: 0.4, color: "#34495e" }
        ];

        function displayFakeEmotions() {
            const emotionScoreCon = document.getElementById('emotionScoreCon');
            emotionScoreCon.innerHTML = '';
            fakeEmotions.forEach(e => {
                const bar = document.createElement('div');
                bar.className = 'emotion-bar';
                bar.innerHTML = `
                    <div class="emotion-bar-inner" style="width:${e.score*100}%;background:${e.color};"></div>
                    <span class="emotion-label">${e.name} (${Math.round(e.score*100)}%)</span>
                `;
                emotionScoreCon.appendChild(bar);
            });
        }

        // Display fake emotions when page loads
        displayFakeEmotions();

        ////////////////////////////////////////////////////////////////////
        // MEDIA DEVICE INITIALIZATION AND MANAGEMENT
        ////////////////////////////////////////////////////////////////////
        // Initialize media devices with enhanced echo cancellation
        async function initializeMedia(includeVideo = false) {
            try {
                const constraints = {
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        // Enhanced audio settings to prevent feedback
                        sampleRate: 48000,
                        channelCount: 1,
                        latency: 0.1,
                        // Additional constraints for better echo cancellation
                        googEchoCancellation: true,
                        googNoiseSuppression: true,
                        googAutoGainControl: true,
                        googHighpassFilter: true
                    },
                    video: includeVideo ? {
                        width: { ideal: 640 },
                        height: { ideal: 480 },
                        frameRate: { ideal: 30 }
                    } : false
                };

                localStream = await navigator.mediaDevices.getUserMedia(constraints);
                
                if (includeVideo) {
                    const localVideo = document.getElementById('localVideo');
                    localVideo.srcObject = localStream;
                }

                // Setup audio context for visualization with lower latency
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    latencyHint: 'interactive',
                    sampleRate: 48000
                });
                
                const source = audioContext.createMediaStreamSource(localStream);
                audioAnalyser = audioContext.createAnalyser();
                audioAnalyser.fftSize = 256;
                audioAnalyser.smoothingTimeConstant = 0.8;
                source.connect(audioAnalyser);

                console.log('Media initialized successfully with enhanced echo cancellation');
                return true;
            } catch (error) {
                console.error('Error accessing media devices:', error);
                addMessageToChat('System', 'Error accessing camera/microphone. Please check permissions and ensure no other app is using the microphone.', 'ai-message');
                return false;
            }
        }

        ////////////////////////////////////////////////////////////////////
        // VIDEO EMOTION ANALYSIS FUNCTIONS
        ////////////////////////////////////////////////////////////////////
        // Start emotion analysis for video calls
        function startEmotionAnalysis() {
            if (!localStream) return;

            const canvas = document.getElementById('emotionCanvas') || document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            canvas.width = 640;
            canvas.height = 480;
            canvas.style.display = 'none';
            
            if (!document.getElementById('emotionCanvas')) {
                document.body.appendChild(canvas);
            }

            emotionInterval = setInterval(async () => {
                if (isVideoCall && localStream) {
                    const video = document.getElementById('localVideo');
                    if (video && video.videoWidth > 0) {
                        // Capture frame from video
                        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                        
                        // Convert to blob and send to backend
                        canvas.toBlob(async (blob) => {
                            if (blob) {
                                await sendFrameToBackend(blob);
                            }
                        }, 'image/jpeg', 0.8);
                    }
                }
            }, 1000); // Analyze every second
        }

        // Send frame to backend for emotion analysis
        async function sendFrameToBackend(frameBlob) {
            try {
                const formData = new FormData();
                formData.append('frame', frameBlob, 'frame.jpg');
                formData.append('csrfmiddlewaretoken', document.querySelector('[name=csrfmiddlewaretoken]')?.value || '');

                const response = await fetch('/analyze-emotion/', {
                    method: 'POST',
                    body: formData
                });

                if (response.ok) {
                    const emotions = await response.json();
                    updateEmotionDisplay(emotions);
                }
            } catch (error) {
                console.error('Error sending frame to backend:', error);
            }
        }

        ////////////////////////////////////////////////////////////////////
        // VRM LIP SYNC AND AUDIO PROCESSING FUNCTIONS
        ////////////////////////////////////////////////////////////////////
        
        let currentAudioElement = null;
        let lipSyncAnalyser = null;
        let lipSyncInterval = null;
        let isLipSyncing = false;

        // Audio diagnostic function for debugging (call from browser console)
        window.debugAudioPipeline = function() {
            console.log('=== AUDIO PIPELINE DIAGNOSTIC ===');
            console.log('Current Audio Element:', currentAudioElement);
            console.log('Is Lip Syncing:', isLipSyncing);
            console.log('Audio Context State:', audioContext ? audioContext.state : 'Not initialized');
            console.log('Is Call Active:', isCallActive);
            console.log('Is AI Speaking:', isAISpeaking);
            
            if (currentAudioElement) {
                const buffered = currentAudioElement.buffered;
                const bufferInfo = [];
                for (let i = 0; i < buffered.length; i++) {
                    bufferInfo.push(`${buffered.start(i).toFixed(2)}-${buffered.end(i).toFixed(2)}s`);
                }
                
                console.log('Audio Details:', {
                    currentTime: currentAudioElement.currentTime.toFixed(2) + 's',
                    duration: (currentAudioElement.duration || 0).toFixed(2) + 's',
                    paused: currentAudioElement.paused,
                    ended: currentAudioElement.ended,
                    readyState: currentAudioElement.readyState,
                    networkState: currentAudioElement.networkState,
                    volume: currentAudioElement.volume,
                    buffered: bufferInfo.join(', ') || 'none',
                    playbackRate: currentAudioElement.playbackRate
                });
            }
            
            if (audioContext) {
                console.log('Audio Context:', {
                    state: audioContext.state,
                    sampleRate: audioContext.sampleRate,
                    currentTime: audioContext.currentTime.toFixed(3) + 's',
                    baseLatency: audioContext.baseLatency || 'unknown',
                    outputLatency: audioContext.outputLatency || 'unknown'
                });
            }
            
            if (lipSyncAnalyser) {
                console.log('Lip Sync Analyser:', {
                    fftSize: lipSyncAnalyser.fftSize,
                    frequencyBinCount: lipSyncAnalyser.frequencyBinCount,
                    smoothingTimeConstant: lipSyncAnalyser.smoothingTimeConstant,
                    connected: 'Connected to audio pipeline'
                });
            }
            
            console.log('=== END DIAGNOSTIC ===');
        };
        
        // New function to monitor audio buffering in real-time
        window.monitorAudioBuffering = function(duration = 10000) {
            console.log(`Starting audio buffering monitor for ${duration/1000}s...`);
            const startTime = Date.now();
            
            const monitor = setInterval(() => {
                if (!currentAudioElement || Date.now() - startTime > duration) {
                    clearInterval(monitor);
                    console.log('Audio buffering monitor stopped');
                    return;
                }
                
                const buffered = currentAudioElement.buffered;
                const currentTime = currentAudioElement.currentTime;
                const duration = currentAudioElement.duration || 0;
                
                let bufferedRanges = [];
                for (let i = 0; i < buffered.length; i++) {
                    bufferedRanges.push(`${buffered.start(i).toFixed(1)}-${buffered.end(i).toFixed(1)}s`);
                }
                
                console.log(`[BUFFER] Time: ${currentTime.toFixed(1)}/${duration.toFixed(1)}s, Buffered: [${bufferedRanges.join(', ')}], State: ${currentAudioElement.readyState}`);
            }, 1000);
        };

        // Base64 to Blob conversion
        function base64ToBlob(base64, mime) {
            const byteChars = atob(base64);
            const byteNumbers = new Array(byteChars.length);
            for (let i = 0; i < byteChars.length; i++) {
                byteNumbers[i] = byteChars.charCodeAt(i);
            }
            const byteArray = new Uint8Array(byteNumbers);
            return new Blob([byteArray], {type: mime});
        }

        // Play AI audio and trigger lip sync (improved version with validation and retry)
        function playAudioWithLipSync(base64Audio) {
            try {
                console.log('Starting AI audio playback with lip sync...');
                
                // Stop any currently playing audio to prevent overlap
                if (currentAudioElement) {
                    console.log('Stopping previous audio before starting new one');
                    currentAudioElement.pause();
                    currentAudioElement.currentTime = 0;
                    currentAudioElement = null;
                }
                
                // Stop any existing lip sync
                if (isLipSyncing) {
                    console.log('Stopping previous lip sync before starting new one');
                    stopLipSync();
                }
                
                // Validate base64 audio with more thorough checks
                if (!base64Audio || typeof base64Audio !== 'string') {
                    console.error('Invalid audio data: not a string');
                    return;
                }
                
                if (base64Audio.length < 500) {
                    console.warn(`Audio data very short (${base64Audio.length} chars) - may be incomplete, but attempting playback`);
                }
                
                // Check for valid base64 content
                try {
                    // Try to decode a small portion to validate
                    const testDecode = atob(base64Audio.substring(0, Math.min(100, base64Audio.length)));
                    console.log('Base64 audio validation passed');
                } catch (e) {
                    console.error('Invalid base64 audio data:', e);
                    return;
                }
                
                // Determine audio format from base64 header with better detection
                let mimeType = 'audio/wav';
                const header = base64Audio.substring(0, 100).toLowerCase();
                
                if (header.includes('mpeg') || header.includes('mp3') || header.includes('/mp')) {
                    mimeType = 'audio/mpeg';
                } else if (header.includes('ogg') || header.includes('vorbis')) {
                    mimeType = 'audio/ogg';
                } else if (header.includes('webm')) {
                    mimeType = 'audio/webm';
                } else if (header.includes('wav') || header.includes('riff')) {
                    mimeType = 'audio/wav';
                } else if (header.includes('mp4') || header.includes('aac')) {
                    mimeType = 'audio/mp4';
                }
                
                console.log(`Detected audio format: ${mimeType}, data size: ${base64Audio.length} chars`);
                
                // Convert to blob with validation
                let audioBlob;
                try {
                    audioBlob = base64ToBlob(base64Audio, mimeType);
                    if (audioBlob.size < 1000) {
                        console.warn(`Audio blob very small (${audioBlob.size} bytes) - may be incomplete`);
                    }
                    console.log(`Audio blob created successfully: ${audioBlob.size} bytes`);
                } catch (e) {
                    console.error('Failed to create audio blob:', e);
                    return;
                }
                
                const audioUrl = URL.createObjectURL(audioBlob);
                
                // Create audio element with optimized settings
                currentAudioElement = new Audio();
                currentAudioElement.src = audioUrl;
                currentAudioElement.volume = 0.8;
                currentAudioElement.preload = 'auto';
                currentAudioElement.crossOrigin = 'anonymous';
                
                console.log(`Audio element created with ${mimeType}, blob size: ${audioBlob.size} bytes`);
                
                // Setup audio context for lip sync analysis
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)({
                        latencyHint: 'interactive',
                        sampleRate: 48000
                    });
                }
                
                // Ensure audio context is resumed (required by some browsers)
                if (audioContext.state === 'suspended') {
                    audioContext.resume().then(() => {
                        console.log('Audio context resumed successfully');
                    }).catch(e => {
                        console.error('Failed to resume audio context:', e);
                    });
                }
                
                // Create analyser for lip sync
                lipSyncAnalyser = audioContext.createAnalyser();
                lipSyncAnalyser.fftSize = 512; // Increased for better frequency resolution
                lipSyncAnalyser.smoothingTimeConstant = 0.2; // Faster response
                lipSyncAnalyser.minDecibels = -90;
                lipSyncAnalyser.maxDecibels = -10;
                
                // Enhanced audio event listeners with retry logic
                let audioRetryCount = 0;
                const maxRetries = 3;
                
                currentAudioElement.addEventListener('canplaythrough', () => {
                    try {
                        // Connect audio to analyser
                        const audioSource = audioContext.createMediaElementSource(currentAudioElement);
                        audioSource.connect(lipSyncAnalyser);
                        lipSyncAnalyser.connect(audioContext.destination);
                        
                        console.log('Audio pipeline connected for lip sync - ready to play');
                    } catch (e) {
                        console.error('Error connecting audio pipeline:', e);
                    }
                });
                
                // Start lip sync when audio plays
                currentAudioElement.addEventListener('play', () => {
                    console.log('AI audio started playing - beginning lip sync');
                    startLipSync();
                });
                
                // Stop lip sync when audio ends naturally
                currentAudioElement.addEventListener('ended', () => {
                    const duration = currentAudioElement.duration;
                    const currentTime = currentAudioElement.currentTime;
                    console.log(`AI audio ended naturally - duration: ${duration}s, played to: ${currentTime}s`);
                    
                    if (Math.abs(duration - currentTime) > 0.5) {
                        console.warn(`Possible audio cutoff detected! Duration: ${duration}s, Current: ${currentTime}s`);
                    } else {
                        console.log('Audio played completely - no cutoff detected');
                    }
                    
                    stopLipSync();
                    URL.revokeObjectURL(audioUrl);
                });
                
                // Handle audio errors with retry
                currentAudioElement.addEventListener('error', (e) => {
                    console.error('Audio playback error:', e);
                    console.error('Error details:', currentAudioElement.error);
                    console.error('Tried mime type:', mimeType);
                    console.error('Audio blob size:', audioBlob.size);
                    
                    if (audioRetryCount < maxRetries) {
                        audioRetryCount++;
                        console.log(`Retrying audio playback (attempt ${audioRetryCount}/${maxRetries})`);
                        
                        // Try with different mime type
                        const fallbackMimeTypes = ['audio/wav', 'audio/mpeg', 'audio/ogg', 'audio/webm'];
                        const fallbackMime = fallbackMimeTypes[audioRetryCount - 1] || 'audio/wav';
                        
                        setTimeout(() => {
                            console.log(`Retry with fallback mime type: ${fallbackMime}`);
                            const retryBlob = base64ToBlob(base64Audio, fallbackMime);
                            const retryUrl = URL.createObjectURL(retryBlob);
                            currentAudioElement.src = retryUrl;
                            currentAudioElement.load();
                        }, 500);
                    } else {
                        console.error('Audio playback failed after all retries');
                        stopLipSync();
                        URL.revokeObjectURL(audioUrl);
                    }
                });
                
                // Handle audio loading states with detailed logging
                currentAudioElement.addEventListener('loadstart', () => {
                    console.log('Audio loading started');
                });
                
                currentAudioElement.addEventListener('loadeddata', () => {
                    console.log('Audio data loaded, duration:', currentAudioElement.duration || 'unknown');
                });
                
                currentAudioElement.addEventListener('loadedmetadata', () => {
                    const duration = currentAudioElement.duration;
                    console.log(`Audio metadata loaded - duration: ${duration ? duration.toFixed(2) : 'unknown'}s`);
                    
                    if (duration && duration < 0.5) {
                        console.warn(`Audio duration very short (${duration}s) - may be incomplete`);
                    }
                });
                
                currentAudioElement.addEventListener('canplay', () => {
                    console.log('Audio can start playing');
                });
                
                currentAudioElement.addEventListener('timeupdate', () => {
                    if (currentAudioElement) {
                        const current = currentAudioElement.currentTime;
                        const duration = currentAudioElement.duration;
                        // Log progress every 2 seconds to avoid spam
                        if (Math.floor(current) % 2 === 0 && Math.floor(current) !== Math.floor(current - 0.1)) {
                            console.log(`Audio progress: ${current.toFixed(1)}s / ${duration ? duration.toFixed(1) : '?'}s`);
                        }
                    }
                });
                
                currentAudioElement.addEventListener('pause', () => {
                    console.log('Audio paused at:', currentAudioElement.currentTime);
                });
                
                currentAudioElement.addEventListener('stalled', () => {
                    console.warn('Audio playback stalled - network or decoding issue');
                });
                
                currentAudioElement.addEventListener('waiting', () => {
                    console.warn('Audio waiting for more data');
                });
                
                // Play the audio
                const playPromise = currentAudioElement.play();
                if (playPromise !== undefined) {
                    playPromise.catch(error => {
                        console.error('Failed to play AI audio:', error);
                        console.error('Audio format:', mimeType);
                        stopLipSync();
                    });
                }
                
            } catch (error) {
                console.error('Error in playAudioWithLipSync:', error);
                stopLipSync();
            }
        }

        // Legacy function name for compatibility
        function playAIAudioWithLipSync(base64Audio) {
            // Backend already combines chunks, play directly
            playAudioWithLipSync(base64Audio);
        }

        // Start real-time lip sync analysis
        function startLipSync() {
            if (!currentVrm || !lipSyncAnalyser || isLipSyncing) return;
            
            isLipSyncing = true;
            const bufferLength = lipSyncAnalyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            
            console.log('Lip sync started for VRM model');
            
            function updateLipSync() {
                if (!isLipSyncing || !lipSyncAnalyser) return;
                
                try {
                    lipSyncAnalyser.getByteFrequencyData(dataArray);
                    
                    // Analyze frequency ranges for different mouth shapes
                    const lowFreq = getAverageVolume(dataArray, 0, 5);        // 'ou', 'oh' sounds
                    const midFreq = getAverageVolume(dataArray, 5, 15);       // 'aa' sounds  
                    const highFreq = getAverageVolume(dataArray, 15, 30);     // 'ee', 'ih' sounds
                    const totalVolume = getAverageVolume(dataArray, 0, 50);   // Overall volume
                    
                    // Normalize volumes (0-1 range)
                    const normalizedLow = Math.min(lowFreq / 128, 1);
                    const normalizedMid = Math.min(midFreq / 128, 1);
                    const normalizedHigh = Math.min(highFreq / 128, 1);
                    const normalizedTotal = Math.min(totalVolume / 128, 1);
                    
                    // Apply mouth shapes based on frequency analysis
                    if (currentVrm && normalizedTotal > 0.1) { // Only animate if there's significant audio
                        // Reset all mouth shapes first
                        currentVrm.expressionManager.setValue('aa', 0);
                        currentVrm.expressionManager.setValue('ee', 0);
                        currentVrm.expressionManager.setValue('ou', 0);
                        currentVrm.expressionManager.setValue('oh', 0);
                        currentVrm.expressionManager.setValue('ih', 0);
                        
                        // Apply dominant mouth shape based on frequency analysis
                        if (normalizedLow > normalizedMid && normalizedLow > normalizedHigh) {
                            // Low frequencies - 'ou' and 'oh' sounds
                            currentVrm.expressionManager.setValue('ou', normalizedLow * 0.8);
                            currentVrm.expressionManager.setValue('oh', normalizedLow * 0.6);
                        } else if (normalizedHigh > normalizedMid) {
                            // High frequencies - 'ee' and 'ih' sounds  
                            currentVrm.expressionManager.setValue('ee', normalizedHigh * 0.7);
                            currentVrm.expressionManager.setValue('ih', normalizedHigh * 0.5);
                        } else {
                            // Mid frequencies - 'aa' sounds (default talking)
                            currentVrm.expressionManager.setValue('aa', normalizedMid * 0.8);
                        }
                        
                        // Add subtle random variation for more natural movement
                        const randomFactor = (Math.random() - 0.5) * 0.1;
                        const currentValue = currentVrm.expressionManager.getValue('aa') || 0;
                        currentVrm.expressionManager.setValue('aa', Math.max(0, Math.min(1, currentValue + randomFactor)));
                    } else {
                        // No significant audio - return to neutral
                        currentVrm.expressionManager.setValue('aa', 0);
                        currentVrm.expressionManager.setValue('ee', 0);
                        currentVrm.expressionManager.setValue('ou', 0);
                        currentVrm.expressionManager.setValue('oh', 0);
                        currentVrm.expressionManager.setValue('ih', 0);
                    }
                    
                    // Continue animation loop
                    if (isLipSyncing) {
                        requestAnimationFrame(updateLipSync);
                    }
                    
                } catch (error) {
                    console.error('Error in lip sync update:', error);
                    stopLipSync();
                }
            }
            
            updateLipSync();
        }

        // Stop lip sync and reset mouth to neutral
        function stopLipSync() {
            console.log('Stopping lip sync...');
            isLipSyncing = false;
            
            if (currentVrm) {
                // Reset all mouth shapes to neutral
                currentVrm.expressionManager.setValue('aa', 0);
                currentVrm.expressionManager.setValue('ee', 0);
                currentVrm.expressionManager.setValue('ou', 0);
                currentVrm.expressionManager.setValue('oh', 0);
                currentVrm.expressionManager.setValue('ih', 0);
                console.log('VRM mouth reset to neutral position');
            }
            
            // Clean up audio resources
            if (currentAudioElement) {
                // Don't pause here - let it finish naturally
                console.log('Audio element cleanup - audio will finish naturally');
                currentAudioElement = null;
            }
            
            if (lipSyncAnalyser) {
                try {
                    lipSyncAnalyser.disconnect();
                } catch (e) {
                    console.log('Analyser already disconnected');
                }
                lipSyncAnalyser = null;
            }
            
            console.log('Lip sync stopped and cleaned up completely');
        }

        // Helper function to get average volume in a frequency range
        function getAverageVolume(array, startIndex, endIndex) {
            let sum = 0;
            const count = endIndex - startIndex;
            
            for (let i = startIndex; i < endIndex && i < array.length; i++) {
                sum += array[i];
            }
            
            return count > 0 ? sum / count : 0;
        }

        // Fallback lip sync for text-based responses (when no audio available)
        function animateTextBasedLipSync(text) {
            if (!currentVrm || !text) return;
            
            console.log('Starting text-based lip sync animation for:', text.substring(0, 50) + '...');
            
            // Estimate speaking duration based on text length (average 150 words per minute)
            const estimatedDuration = (text.split(' ').length / 150) * 60 * 1000; // milliseconds
            const animationSteps = Math.max(10, Math.min(50, estimatedDuration / 100)); // 10-50 steps
            
            let currentStep = 0;
            
            function animateStep() {
                if (currentStep >= animationSteps || !currentVrm) {
                    // Animation complete - reset to neutral
                    stopLipSync();
                    return;
                }
                
                // Create varied mouth movements based on text characters
                const progress = currentStep / animationSteps;
                const char = text[Math.floor(progress * text.length)]?.toLowerCase() || 'a';
                
                // Reset all shapes
                currentVrm.expressionManager.setValue('aa', 0);
                currentVrm.expressionManager.setValue('ee', 0);
                currentVrm.expressionManager.setValue('ou', 0);
                currentVrm.expressionManager.setValue('oh', 0);
                currentVrm.expressionManager.setValue('ih', 0);
                
                // Apply mouth shape based on character
                const intensity = 0.3 + (Math.sin(progress * Math.PI) * 0.4); // Smooth intensity curve
                
                if ('aeiou'.includes(char)) {
                    if (char === 'a') currentVrm.expressionManager.setValue('aa', intensity);
                    else if (char === 'e') currentVrm.expressionManager.setValue('ee', intensity);
                    else if (char === 'i') currentVrm.expressionManager.setValue('ih', intensity);
                    else if (char === 'o') currentVrm.expressionManager.setValue('oh', intensity);
                    else if (char === 'u') currentVrm.expressionManager.setValue('ou', intensity);
                } else {
                    // Consonants - default to 'aa' with variation
                    currentVrm.expressionManager.setValue('aa', intensity * 0.6);
                }
                
                currentStep++;
                setTimeout(animateStep, estimatedDuration / animationSteps);
            }
            
            animateStep();
        }

        ////////////////////////////////////////////////////////////////////
        // HUME EVI AUDIO STREAMING AND VOICE DETECTION
        ////////////////////////////////////////////////////////////////////
        // Send audio data to backend - Modified for Hume EVI integration
        function startAudioStreaming() {
            if (!localStream) return;

            console.log('Starting audio streaming with Hume EVI integration...');
            
            // With Hume EVI, the backend handles microphone input directly
            // We'll focus on visual feedback and polling for responses
            
            // Start enhanced voice activity detection for visual feedback
            startEnhancedVoiceActivityDetection();
            
            // Start polling for AI responses from Hume EVI
            startPollingAIResponses();
            
            // Add user feedback
            addMessageToChat('System', 'Hume EVI connected. Speak naturally - AI will respond in real-time!', 'ai-message');
        }

        // Enhanced Voice Activity Detection for visual feedback only
        function startEnhancedVoiceActivityDetection() {
            if (!audioAnalyser) return;
            
            const bufferLength = audioAnalyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            let silenceCount = 0;
            let speechCount = 0;
            const silenceThreshold = 15;
            const speechThreshold = 3;
            let isSpeaking = false;
            
            function detectVoice() {
                // Stop detection if call is not active or audioAnalyser is null
                if (!isCallActive || !audioAnalyser) {
                    console.log('Voice detection stopped - call inactive or audioAnalyser destroyed');
                    return;
                }
                
                try {
                    audioAnalyser.getByteFrequencyData(dataArray);
                    
                    // Calculate average volume with frequency weighting
                    const weightedAverage = dataArray.reduce((sum, value, index) => {
                        const weight = index < bufferLength / 3 ? 2 : 1;
                        return sum + (value * weight);
                    }, 0) / (bufferLength * 1.5);
                    
                    const voiceThreshold = Math.max(15, weightedAverage * 0.3);
                    
                    if (weightedAverage > voiceThreshold) {
                        speechCount++;
                        silenceCount = 0;
                    } else {
                        silenceCount++;
                        speechCount = 0;
                    }
                    
                    // Update speaking state for visual feedback
                    const wasSpeaking = isSpeaking;
                    if (speechCount >= speechThreshold && !isSpeaking) {
                        isSpeaking = true;
                        console.log('Speech detected - Visual feedback only (Hume EVI handles audio)');
                        updateSpeechIndicator(true);
                    } else if (silenceCount >= silenceThreshold && isSpeaking) {
                        isSpeaking = false;
                        console.log('Silence detected - Visual feedback only');
                        updateSpeechIndicator(false);
                    }
                    
                    // Update audio visualization
                    updateAudioVisualization(dataArray, isSpeaking);
                    
                    // Continue only if call is still active
                    if (isCallActive) {
                        requestAnimationFrame(detectVoice);
                    }
                } catch (error) {
                    console.error('Error in voice detection:', error);
                    // Stop on error
                    return;
                }
            }
            
            detectVoice();
        }

        // Poll for AI responses from Hume EVI
        function startPollingAIResponses() {
            let pollingInterval = setInterval(async () => {
                if (!isCallActive) {
                    clearInterval(pollingInterval);
                    return;
                }
                
                try {
                    const response = await fetch('/poll-ai-response/', {
                        method: 'GET',
                    });
                    
                    if (response.ok) {
                        const result = await response.json();
                        
                        if (result.has_response && result.data) {
                            const responseData = result.data;
                            
                            // Handle different types of responses
                            if (responseData.response) {
                                // AI text response - add to chat and trigger text-based lip sync
                                addMessageToChat('AI', responseData.response, 'ai-message');
                                
                                // Trigger text-based lip sync as fallback
                                animateTextBasedLipSync(responseData.response);
                                
                                // Update emotion display if available
                                if (responseData.voice_analysis && responseData.voice_analysis.emotions) {
                                    updateEmotionDisplayFromVoice(responseData.voice_analysis.emotions);
                                }
                                
                                console.log('AI response received from Hume EVI:', responseData.response);
                                
                            } else if (responseData.type === 'audio_output' && responseData.audio_data) {
                                // AI audio output - play with real-time lip sync
                                const audioSize = responseData.audio_data.length;
                                const chunkCount = responseData.chunk_count || 1;
                                const validChunks = responseData.valid_chunks || chunkCount;
                                const streamDuration = responseData.stream_duration || 0;
                                
                                console.log(`[FRONTEND] AI audio received:`, {
                                    totalChunks: chunkCount,
                                    validChunks: validChunks,
                                    audioSize: audioSize,
                                    streamDuration: streamDuration.toFixed(3) + 's',
                                    chunkEfficiency: (validChunks / chunkCount * 100).toFixed(1) + '%'
                                });
                                
                                // Check for potential buffering issues
                                if (streamDuration > 2.0) {
                                    console.warn(`[BUFFERING] Long stream duration: ${streamDuration.toFixed(3)}s - possible buffering delays`);
                                }
                                if (validChunks < chunkCount) {
                                    console.warn(`[BUFFERING] Dropped chunks: ${chunkCount - validChunks}/${chunkCount} - audio quality may be affected`);
                                }
                                if (audioSize < 1000) {
                                    console.warn(`[BUFFERING] Small audio size: ${audioSize} chars - audio may be very short`);
                                }
                                
                                playAIAudioWithLipSync(responseData.audio_data);
                                
                            } else if (responseData.user_message) {
                                // User speech transcription
                                addMessageToChat('You', responseData.user_message, 'user-message');
                                console.log('User speech transcribed:', responseData.user_message);
                                
                                // Update emotion display with user emotions (ONLY USER EMOTIONS)
                                if (responseData.emotions && responseData.emotions.length > 0) {
                                    updateEmotionDisplayFromVoice(responseData.emotions);
                                    console.log('User emotions updated:', responseData.emotions.map(e => `${e.name}: ${Math.round(e.score*100)}%`));
                                }
                                
                            } else if (responseData.type === 'auto_terminate') {
                                // Auto-termination triggered by goodbye
                                addMessageToChat('System', responseData.message, 'ai-message');
                                addMessageToChat('You', `"${responseData.user_message}"`, 'user-message');
                                console.log('Auto-termination triggered by goodbye:', responseData.user_message);
                                
                                // Automatically stop the call after a brief delay
                                setTimeout(() => {
                                    addMessageToChat('AI', 'Goodbye! Have a wonderful day! üëã', 'ai-message');
                                    setTimeout(() => {
                                        stopCall();
                                    }, 2000);
                                }, 1000);
                                
                            } else if (responseData.type === 'ai_speaking_start') {
                                // AI started speaking - mute microphone sensitivity and start lip sync preparation
                                isAISpeaking = true;
                                updateSpeechIndicator(false, true); // false for user, true for AI
                                console.log('AI started speaking - reducing mic sensitivity, preparing for lip sync');
                                
                            } else if (responseData.type === 'ai_speaking_end') {
                                // AI finished speaking - restore microphone sensitivity 
                                // But don't stop lip sync - let audio finish naturally
                                isAISpeaking = false;
                                console.log('AI finished speaking - restoring mic sensitivity, letting audio finish naturally');
                            }
                        }
                    }
                } catch (error) {
                    console.error('Error polling AI response:', error);
                }
            }, 200); // Poll more frequently for real-time feel but not too aggressive
        }

        ////////////////////////////////////////////////////////////////////
        // UI VISUAL FEEDBACK AND INDICATORS
        ////////////////////////////////////////////////////////////////////
        // Update speech indicator in UI
        function updateSpeechIndicator(isSpeaking, aiSpeaking = false) {
            const statusElement = document.getElementById('audioCallStatus') || document.getElementById('videoCallStatus');
            const speechIndicator = document.getElementById('speechIndicator') || document.getElementById('speechIndicatorVideo');
            
            if (statusElement) {
                if (aiSpeaking) {
                    statusElement.textContent = 'Hume EVI - AI is speaking...';
                    statusElement.style.background = '#9b59b6'; // Purple for AI speaking
                } else if (isSpeaking) {
                    statusElement.textContent = 'Hume EVI Active - User speaking...';
                    statusElement.style.background = '#e74c3c'; // Red for user speaking
                } else {
                    statusElement.textContent = 'Hume EVI Active - Listening for voice...';
                    statusElement.style.background = '#2ecc71'; // Green for listening
                }
            }
            
            if (speechIndicator) {
                if (aiSpeaking) {
                    speechIndicator.style.display = 'block';
                    speechIndicator.textContent = 'ü§ñ AI Speaking...';
                    speechIndicator.classList.add('speaking');
                    speechIndicator.style.background = 'rgba(155, 89, 182, 0.9)';
                } else if (isSpeaking) {
                    speechIndicator.style.display = 'block';
                    speechIndicator.textContent = 'üéôÔ∏è You are speaking...';
                    speechIndicator.classList.add('speaking');
                    speechIndicator.style.background = 'rgba(231, 76, 60, 0.9)';
                } else {
                    speechIndicator.style.display = 'none';
                    speechIndicator.classList.remove('speaking');
                }
            }a
        }

        // Enhanced audio visualization with voice activity indication
        function updateAudioVisualization(dataArray, isSpeaking) {
            const canvas = document.getElementById('audioVisualization');
            if (!canvas) return;
            
            const ctx = canvas.getContext('2d');
            const bufferLength = dataArray.length;

            // Clear canvas with color based on speech activity
            ctx.fillStyle = isSpeaking ? '#1a1a1a' : '#333';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            const barWidth = (canvas.width / bufferLength) * 2.5;
            let barHeight;
            let x = 0;

            for (let i = 0; i < bufferLength; i++) {
                barHeight = (dataArray[i] / 255) * canvas.height;

                // Color bars based on speech activity and Hume EVI status
                if (isSpeaking) {
                    // Animated colors for active speech
                    const hue = (Date.now() / 20 + i * 2) % 360;
                    ctx.fillStyle = `hsl(${hue}, 70%, ${Math.min(80, barHeight/2 + 40)}%)`;
                } else {
                    ctx.fillStyle = `rgb(${Math.min(255, barHeight + 100)}, 50, 50)`;
                }
                
                ctx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
                x += barWidth + 1;
            }
            
            // Add "HUME EVI" text overlay
            ctx.fillStyle = isSpeaking ? '#fff' : '#aaa';
            ctx.font = 'bold 12px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('HUME EVI', canvas.width / 2, 20);
        }

        ////////////////////////////////////////////////////////////////////
        // LEGACY AUDIO PROCESSING (KEPT FOR COMPATIBILITY)
        ////////////////////////////////////////////////////////////////////
        // Send audio to backend for processing
        async function sendAudioToBackend(audioBlob) {
            try {
                console.log('Sending audio blob, size:', audioBlob.size);
                
                if (audioBlob.size === 0) {
                    console.log('Audio blob is empty, skipping');
                    return;
                }
                
                const formData = new FormData();
                formData.append('audio', audioBlob, 'audio.webm');
                
                // Get CSRF token
                const csrfToken = document.querySelector('[name=csrfmiddlewaretoken]')?.value;
                if (csrfToken) {
                    formData.append('csrfmiddlewaretoken', csrfToken);
                }

                const response = await fetch('/process-audio/', {
                    method: 'POST',
                    body: formData,
                });

                console.log('Audio response status:', response.status);

                if (response.ok) {
                    const result = await response.json();
                    console.log('Audio processing result:', result);
                    
                    if (result.response) {
                        // Add message to chat
                        addMessageToChat('AI', result.response, 'ai-message');
                        
                        // Speak the response if flag is set
                        if (result.should_speak && !isAISpeaking) {
                            speakResponse(result.response);
                        }
                        
                        // Update emotion display if voice analysis is available
                        if (result.voice_analysis && result.voice_analysis.emotions) {
                            updateEmotionDisplayFromVoice(result.voice_analysis.emotions);
                        }
                    }
                } else {
                    const errorText = await response.text();
                    console.error('Audio processing failed:', response.status, errorText);
                    addMessageToChat('System', `Audio processing failed: ${response.status}`, 'ai-message');
                }
            } catch (error) {
                console.error('Error sending audio to backend:', error);
                addMessageToChat('System', 'Network error during audio processing', 'ai-message');
            }
        }

        ////////////////////////////////////////////////////////////////////
        // EMOTION DISPLAY AND MANAGEMENT
        ////////////////////////////////////////////////////////////////////
        // Update emotion display from voice analysis (USER AUDIO ONLY)
        function updateEmotionDisplayFromVoice(voiceEmotions) {
            console.log('updateEmotionDisplayFromVoice called with:', voiceEmotions);
            const emotionScoreCon = document.getElementById('emotionScoreCon');
            if (!voiceEmotions || voiceEmotions.length === 0) {
                console.log('No voice emotions to display');
                return;
            }

            console.log('Updating emotion display with', voiceEmotions.length, 'emotions');
            emotionScoreCon.innerHTML = '';
            voiceEmotions.forEach(emotion => {
                const bar = document.createElement('div');
                bar.className = 'emotion-bar';
                bar.innerHTML = `
                    <div class="emotion-bar-inner" style="width:${emotion.score*100}%;background:${getEmotionColor(emotion.name)};"></div>
                    <span class="emotion-label">${emotion.name} (${Math.round(emotion.score*100)}%)</span>
                `;
                emotionScoreCon.appendChild(bar);
            });
            
            // Display emotions only - no VRM animation
            if (currentVrm && voiceEmotions.length > 0) {
                console.log('User emotions detected:', voiceEmotions.map(e => `${e.name}: ${Math.round(e.score*100)}%`));
            }
        }

        // Update emotion display from video analysis
        function updateEmotionDisplay(emotions) {
            const emotionScoreCon = document.getElementById('emotionScoreCon');
            if (!emotions || emotions.length === 0) return;

            emotionScoreCon.innerHTML = '';
            emotions.forEach(emotion => {
                const bar = document.createElement('div');
                bar.className = 'emotion-bar';
                bar.innerHTML = `
                    <div class="emotion-bar-inner" style="width:${emotion.score*100}%;background:${getEmotionColor(emotion.name)};"></div>
                    <span class="emotion-label">${emotion.name} (${Math.round(emotion.score*100)}%)</span>
                `;
                emotionScoreCon.appendChild(bar);
            });
            
            // Display emotions only - no VRM animation
            if (currentVrm && emotions.length > 0) {
                console.log('Emotions displayed for VRM model (no animation)');
            }
        }

        // Get color for emotion types
        function getEmotionColor(emotionName) {
            const colors = {
                'happy': '#2ecc71',
                'sad': '#3498db',
                'angry': '#e74c3c',
                'surprised': '#f1c40f',
                'fear': '#9b59b6',
                'disgust': '#95a5a6',
                'neutral': '#34495e'
            };
            return colors[emotionName.toLowerCase()] || '#95a5a6';
        }

        // Clear fake emotions when real call starts
        function clearFakeEmotions() {
            const emotionScoreCon = document.getElementById('emotionScoreCon');
            emotionScoreCon.innerHTML = '<div style="color: #fff; font-size: 0.9vw; text-align: center; padding: 1vw;">Waiting for real-time emotion analysis...</div>';
            console.log('Fake emotions cleared - waiting for real user emotions');
        }

        ////////////////////////////////////////////////////////////////////
        // CHAT AND MESSAGE HANDLING
        ////////////////////////////////////////////////////////////////////
        // Add message to chat display
        function addMessageToChat(sender, message, className) {
            const textDisplay = document.getElementById('textDisplay');
            const messageDiv = document.createElement('div');
            messageDiv.className = className;
            messageDiv.innerHTML = `<strong>${sender}:</strong> ${message}`;
            textDisplay.appendChild(messageDiv);
            textDisplay.scrollTop = textDisplay.scrollHeight;
        }

        ////////////////////////////////////////////////////////////////////
        // CALL LIFECYCLE MANAGEMENT
        ////////////////////////////////////////////////////////////////////
        // Audio visualization starter (compatibility function)
        function startAudioVisualization() {
            // Audio visualization is now handled by updateAudioVisualization
            // called from startEnhancedVoiceActivityDetection
            console.log('Audio visualization started via enhanced voice activity detection');
        }

        // Start video call
        async function startVideoCall() {
            const success = await initializeMedia(true);
            if (!success) return;

            isCallActive = true;
            isVideoCall = true;
            showVideoCall();
            
            // Clear fake emotions and prepare for real user emotions
            clearFakeEmotions();
            
            addMessageToChat('System', 'Video call started. AI emotion analysis activated.', 'ai-message');
            
            startEmotionAnalysis();
            
            // Initialize AI backend connection (this handles both Hume EVI and audio streaming)
            await initializeAIBackend();
        }

        // Start audio call
        async function startAudioCall() {
            console.log('Starting audio call with Hume EVI...');
            const success = await initializeMedia(false);
            if (!success) {
                console.log('Failed to initialize media');
                return;
            }

            isCallActive = true;
            isVideoCall = false;
            showNormalCall();
            
            // Clear fake emotions and prepare for real user emotions
            clearFakeEmotions();
            
            console.log('Audio call UI shown, starting Hume EVI integration...');
            addMessageToChat('System', 'Audio call started. Hume EVI real-time voice analysis activated.', 'ai-message');
            
            startAudioVisualization();
            
            // Initialize AI backend connection (this handles both Hume EVI and audio streaming)
            await initializeAIBackend();
            console.log('Hume EVI audio call fully initialized');
        }

        // Initialize AI backend connection
        async function initializeAIBackend() {
            try {
                const response = await fetch('/initialize-ai/', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'X-CSRFToken': document.querySelector('[name=csrfmiddlewaretoken]')?.value || ''
                    },
                    body: JSON.stringify({
                        'call_type': isVideoCall ? 'video' : 'audio'
                    })
                });

                if (response.ok) {
                    const result = await response.json();
                    addMessageToChat('AI', result.message || 'Hume EVI system initialized and ready for natural conversation!', 'ai-message');
                    
                    // Start audio streaming and voice detection after backend initialization
                    startAudioStreaming();
                    
                    // Add instructions for user
                    setTimeout(() => {
                        addMessageToChat('AI', 'Hi! I\'m powered by Hume EVI. Just speak naturally and I\'ll respond in real-time with emotion-aware conversation. How are you feeling today?', 'ai-message');
                    }, 1000);
                    
                    // Add audio feedback prevention tip
                    setTimeout(() => {
                        addMessageToChat('System', 'üí° Tip: Use headphones to prevent audio feedback between your microphone and speakers for the best experience!', 'ai-message');
                    }, 2000);
                    
                    // Add goodbye instruction
                    setTimeout(() => {
                        addMessageToChat('System', 'üëã Say "goodbye", "bye", or "end call" anytime to automatically end the conversation.', 'ai-message');
                    }, 3000);
                }
            } catch (error) {
                console.error('Error initializing AI backend:', error);
                addMessageToChat('System', 'Failed to connect to Hume EVI backend.', 'ai-message');
            }
        }

        // Stop call - Enhanced to properly terminate Hume EVI and stop all audio processing
        async function stopCall() {
            console.log('Stopping call and terminating all connections...');
            isCallActive = false;
            
            // Stop frontend media streams first
            if (localStream) {
                localStream.getTracks().forEach(track => {
                    track.stop();
                    console.log(`Stopped ${track.kind} track`);
                });
                localStream = null;
            }
            
            // Stop intervals and timers
            if (emotionInterval) {
                clearInterval(emotionInterval);
                emotionInterval = null;
            }
            
            // Close audio context completely
            if (audioContext && audioContext.state !== 'closed') {
                try {
                    // Disconnect all nodes before closing
                    if (audioAnalyser) {
                        audioAnalyser.disconnect();
                        audioAnalyser = null;
                    }
                    await audioContext.close();
                    audioContext = null;
                    console.log('Audio context closed and disconnected');
                } catch (error) {
                    console.error('Error closing audio context:', error);
                }
            }
            
            // Reset audio-related variables
            audioAnalyser = null;
            mediaRecorder = null;
            
            // Stop any active lip sync
            stopLipSync();
            
            // Notify backend to stop AI session and close Hume EVI connection
            try {
                console.log('Sending stop request to backend...');
                const response = await fetch('/stop-ai-session/', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'X-CSRFToken': document.querySelector('[name=csrfmiddlewaretoken]')?.value || ''
                    },
                    body: JSON.stringify({
                        'action': 'stop_session',
                        'force_stop': true
                    })
                });

                if (response.ok) {
                    const result = await response.json();
                    addMessageToChat('System', result.message || 'Call ended. Hume EVI session terminated.', 'ai-message');
                    console.log('Backend confirmed session stopped');
                } else {
                    const errorText = await response.text();
                    console.error('Failed to stop backend session:', errorText);
                    addMessageToChat('System', 'Call ended. Warning: Backend session may still be active.', 'ai-message');
                }
            } catch (error) {
                console.error('Error stopping AI session:', error);
                addMessageToChat('System', 'Call ended. Network error during cleanup - session may still be running in background.', 'ai-message');
            }
            
            // Reset UI state
            isAISpeaking = false;
            updateSpeechIndicator(false, false);
            
            // Reset global flags completely
            isCallActive = false;
            isVideoCall = false;
            
            // Hide call interface
            hideCallBox();
            
            console.log('Call termination completed - all audio processing stopped');
        }

        ////////////////////////////////////////////////////////////////////
        // UI LAYOUT AND BUTTON CONTROLS
        ////////////////////////////////////////////////////////////////////
        const callBox = document.getElementById('callBox');
        const audioCallBtn = document.getElementById('audioCallBtn');
        const videoCallBtn = document.getElementById('videoCallBtn');

        // Call box visibility controls
        function showCallBox() {
            callBox.style.display = 'flex';
        }
        
        function hideCallBox() {
            callBox.style.display = 'block'; //revert this to none later
        }

        // Call layout switching
        function showVideoCall() {
            showCallBox();
            document.getElementById('videoCallLayout').style.display = 'flex';
            document.getElementById('normalCallLayout').style.display = 'none';
        }
        
        function showNormalCall() {
            showCallBox();
            document.getElementById('videoCallLayout').style.display = 'none';
            document.getElementById('normalCallLayout').style.display = 'flex';
        }

        ////////////////////////////////////////////////////////////////////
        // EVENT HANDLERS AND INITIALIZATION
        ////////////////////////////////////////////////////////////////////
        // Button click handlers with AI initialization
        audioCallBtn.onclick = startAudioCall;
        videoCallBtn.onclick = startVideoCall;

        // Stop call buttons are now handled by form submission to /stop-ai-session/
        // Add form submission handlers to ensure proper cleanup
        document.addEventListener('DOMContentLoaded', function() {
            // Find all stop call forms and add submit handlers
            const stopForms = document.querySelectorAll('form[action="/stop-ai-session/"]');
            stopForms.forEach(form => {
                form.addEventListener('submit', function(e) {
                    // Let the form submit naturally to the backend
                    // But also trigger our frontend cleanup
                    setTimeout(() => {
                        stopCall();
                    }, 100);
                });
            });
        });

        // Initialize UI state
        hideCallBox(); // Hide call box by default
        document.getElementById('closeCallBox').onclick = hideCallBox;

    </script>
</body>
</html>